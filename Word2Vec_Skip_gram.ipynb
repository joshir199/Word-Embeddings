{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrqoOp2Rbbmy"
      },
      "outputs": [],
      "source": [
        "!pip install -q tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports of libraries.\n",
        "import io\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "metadata": {
        "id": "v2uc5-hhdM54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.version.VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An1VOZOPdWd8",
        "outputId": "cb180d3a-70a5-4974-d7b3-e6662825f193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "metadata": {
        "id": "gEww56zideBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize a normal sentence\n",
        "sentence = \"The blue moon appeared in the sky\"\n",
        "tokens = list(sentence.lower().split())\n",
        "print(len(tokens)), tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz3FIyChdeSp",
        "outputId": "ddba57a8-c807-4bad-e94e-bdbf0f6ffaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, ['the', 'blue', 'moon', 'appeared', 'in', 'the', 'sky'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating vocabulary to save words mapping in the sentence\n",
        "vocab, index = {}, 1 # start indexing from 1\n",
        "vocab['<pad>'] = 0 # add a padding token\n",
        "for token in tokens:\n",
        "  if token not in vocab: # unique words only\n",
        "    vocab[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98b6lFLzeDnQ",
        "outputId": "536ab3d4-cc00-4401-96c8-0ebcec6e4e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<pad>': 0, 'the': 1, 'blue': 2, 'moon': 3, 'appeared': 4, 'in': 5, 'sky': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create reverse mapping of index to words\n",
        "inverse_vocab = {index:token for token, index in vocab.items()}\n",
        "inverse_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-hoELdUeaXr",
        "outputId": "5ec44f8a-c235-489a-81aa-e592dbbdebbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '<pad>', 1: 'the', 2: 'blue', 3: 'moon', 4: 'appeared', 5: 'in', 6: 'sky'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorize the sentence\n",
        "example_sequence = [vocab[token] for token in tokens]\n",
        "example_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4VaJ0xifC7V",
        "outputId": "a322ebc8-b25c-4f60-dc86-375b9b38b9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 1, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating skip-gram for one sentence (sequence of words)\n",
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence,\n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "\n",
        "print(len(positive_skip_grams))\n",
        "#positive_skip_grams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wSR0HKXffCG",
        "outputId": "3b47e117-d26b-4277-dcd4-f8613ee753bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target words - words in focus / context - neighboring words\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF8wZ_VFgb34",
        "outputId": "6ad1a4f1-31ed-4301-b84b-fd017b1aba87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6): (the, sky)\n",
            "(3, 4): (moon, appeared)\n",
            "(2, 4): (blue, appeared)\n",
            "(4, 1): (appeared, the)\n",
            "(1, 5): (the, in)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The skipgrams function returns all positive skip-gram pairs by sliding over a given window span.\n",
        "#To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample\n",
        "# random words from the vocabulary. Use the tf.random.log_uniform_candidate_sampler function to sample num_ns\n",
        "#number of negative samples for a given target word in a window. You can call the funtion on one skip-grams's\n",
        "#target word and pass the context word as true class to exclude it from being sampled.\n",
        "\n",
        "#The training objective of the skip-gram model is to maximize the probability of predicting\n",
        "#context words given the target word For a sequence of words w1, w2, ... wT.\n",
        "\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "num_ns = 4 # number of negative samples per word\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class, # class that should be sampled as 'positive'\n",
        "    num_true=1, # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns, # number of negative context words to sample\n",
        "    unique=True, # all the negative samples should be unique\n",
        "    range_max=vocab_size, # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED, # seed for reproducibility\n",
        "    name=\"negative_sampling\" # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bes33PWvgvJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19645819-77a6-4ad5-ad49-fa8b4653b850"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
            "['blue', 'the', 'appeared', 'moon']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a dimension so you can use concatenation (on the next step).\n",
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "\n",
        "# Concat positive context word with negative sampled words.\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label =  tf.squeeze(label)"
      ],
      "metadata": {
        "id": "kRO7h9f_suK4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtpPdTQst7Q5",
        "outputId": "0dc56eb2-d7ef-43b8-91cf-9838c4ba9e43"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_index    : 1\n",
            "target_word     : the\n",
            "context_indices : [6 2 1 4 3]\n",
            "context_words   : ['sky', 'blue', 'the', 'appeared', 'moon']\n",
            "label           : [1 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"target  :\", target)\n",
        "print(f\"context :\", context )\n",
        "print(f\"label   :\", label )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU0mznCkuHrv",
        "outputId": "b65133c3-8207-42b7-b96d-135cf1ccc610"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target  : tf.Tensor(1, shape=(), dtype=int32)\n",
            "context : tf.Tensor([6 2 1 4 3], shape=(5,), dtype=int64)\n",
            "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make a function which will take all these parameters and generate samples for larger words\n",
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=SEED,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels\n"
      ],
      "metadata": {
        "id": "glwGGJR_u51m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "with open(path_to_file) as f:\n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY2KvQoE3Z3z",
        "outputId": "e473cbcf-89e6-4f6c-abb9-d6ae7b7afd81"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "\n",
        "# We create a custom standardization function to lowercase the text and\n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "metadata": {
        "id": "g8O_QLzJ316X"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHrgHyO34W8Z",
        "outputId": "fd82375f-84c5-4891-b499-be35dc960230"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))\n",
        "\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "metadata": {
        "id": "4M-yNP1L4bGo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AObOZWC4qRO",
        "outputId": "8e363cdc-3a6e-4e62-9a43-873e2cf03ca0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV8Mn5FM4vRi",
        "outputId": "6a370c7f-1b13-45ed-f0c6-fca60f2d1f66"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
            "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DWUuwb947cJ",
        "outputId": "38b6a178-45a8-4dd2-8d7e-fefb15a8225e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32777/32777 [00:23<00:00, 1380.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65429 65429 65429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0Exsxre5Ddc",
        "outputId": "f822118b-7544-4239-be19-bfb5bfc8e8b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int32, name=None), TensorSpec(shape=(1024, 5, 1), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA8Jhw225LAR",
        "outputId": "4c98783b-f27f-4e84-e029-3b6d3e43cce7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int32, name=None), TensorSpec(shape=(1024, 5, 1), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)"
      ],
      "metadata": {
        "id": "LA9SXoS-5OFe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wwEY2A1S5SBS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "IIct8yYn5wtu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "JB7lFYBT5jI7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}